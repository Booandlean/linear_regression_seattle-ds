{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from random import gauss\n",
    "from lin_reg import best_line\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slides [here](https://docs.google.com/presentation/d/1DwCvkgzA0PmdwZJAqD82QJnwcR_AnGcFM74s5QxpDkQ/edit?usp=sharing).\n",
    "\n",
    "The idea of _correlation_ is the simple idea that variables often change _together_. For a simple example, cities with more buses tend to have higher populations.\n",
    "\n",
    "We might observe that, as one variable X increases, so does another Y, OR that as X increases, Y decreases.\n",
    "\n",
    "The _covariance_ describes how two variables co-vary. Note the similarity in the definition to the definition of ordinary variance:\n",
    "\n",
    "## Covariance\n",
    "\n",
    "For two random variables $X$ and $Y$, each with $n$ values:\n",
    "\n",
    "$\\Large\\sigma_{XY} = \\frac{\\Sigma^n_{i = 1}(x_i - \\mu_x)(y_i - \\mu_y)}{n}$ <br/>\n",
    "\n",
    "We can get the covariance function from NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [1, 3, 5]\n",
    "Y = [2, 9, 10]\n",
    "\n",
    "np.cov(X, Y, ddof=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the value of the covariance is very much a function of the values of X and Y, which can make interpretation difficult. What is wanted is a _standardized_ scale for covariance, hence: _correlation_.\n",
    "\n",
    "## Correlation\n",
    "\n",
    "Pearson Correlation:<br/>$\\Large r_P = \\frac{\\Sigma^n_{i = 1}(x_i - \\mu_x)(y_i - \\mu_y)}{\\sqrt{\\Sigma^n_{i = 1}(x_i - \\mu_x)^2\\Sigma^n_{i = 1}(y_i -\\mu_y)^2}}$\n",
    "\n",
    "Note that we are simply standardizing the covariance by the standard deviations of X and Y (the ($n-1$)'s cancel!).\n",
    "\n",
    "$\\bf{Check}$:\n",
    "\n",
    "<details><summary>\n",
    "What happens if X = Y?\n",
    "</summary>\n",
    "Then numerator = denominator and the correlation = 1!\n",
    "</details>\n",
    "<br/>\n",
    "We'll always have $-1 \\leq r \\leq 1$. (This was the point of standardizing by the standard deviations of X and Y.)\n",
    "\n",
    "A correlation of -1 means that X and Y are perfectly negatively correlated, and a correlation of 1 means that X and Y are perfectly positively correlated.\n",
    "\n",
    "NumPy also has a correlation method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(X, Y)[0, 1] == (np.cov(X, Y, ddof=0) / (np.std(X) * np.std(Y)))[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causation\n",
    "\n",
    "_Why_ does it happen that variables correlate? It _may_ be that one is the cause of the other. A city having a high population, for example, probably does have some causal effect on the number of buses that the city has. But this _need not_ be the case, and that is why statisticians are fond of saying that 'correlation is not causation'. An alternative possibility, for example, is that high values of X and Y are _both_ caused by high values of some third factor Z. The size of children's feet, for example, is correlated with their ability to spell, but this is of course NOT because either is a cause of the other. Rather, BOTH are caused by the natural maturing and development of children. As they get older, both their feet and their spelling abilities grow!\n",
    "\n",
    "## Statistical Learning Theory\n",
    "\n",
    "It's important at this point to understand the distinction between dependent and independent variables.\n",
    "\n",
    "Roughly, the independent variable is what can be directly manipulated and the dependent variable is what cannot be (but is nevertheless of great interest). What matters structurally is simply that we understand the dependent variable to be a _function_ of the independent variable(s).\n",
    "\n",
    "This is the proper interpretation of a statistical _model_.\n",
    "\n",
    "Simple idea: We can model correlation with a _line_. As one variable changes, so does the other.\n",
    "\n",
    "This model has two *parameters*: *slope* and *y-intercept*.\n",
    "\n",
    "Unless there's a perfectly (and suspiciously) linear relationship between our predictor(s) and our target, there will  be some sort of **error** or **loss** or **residual**. The best-fit line is constructed by minimizing the sum of the squares of these losses.\n",
    "\n",
    "## Simple Linear Regression\n",
    "\n",
    "The solution for a simple regression best-fit line is as follows:\n",
    "\n",
    "- slope: <br/>$\\Large m = r_P\\frac{\\sigma_y}{\\sigma_x}$\n",
    "\n",
    "- y-intercept:<br/> $\\Large b = \\mu_y - m\\mu_x$\n",
    "\n",
    "Note: The proof of this proceeds by setting the gradient of the loss function equal to 0.\n",
    "\n",
    "Assumptions:\n",
    "\n",
    "1. The relationship between target and predictor(s) is linear. (Of course!)\n",
    "2. The errors are mutually independent. (That is, there is no correlation between any two errors.)\n",
    "3. The errors are normally distributed. (That is, smaller errors are more probable than larger errors, according to the familiar bell curve.)\n",
    "4. The errors are homoskedastic. (That is, the errors have the same variance. The Greek word $\\sigma\\kappa\\epsilon\\delta\\acute{\\alpha}\\nu\\nu\\upsilon\\mu\\iota$ means \"to scatter\".)\n",
    "\n",
    "There is no general requirement that the predictors and the target *themselves* be normally distributed. However: Linear regression can work better if the predictors are normally distributed.\n",
    "\n",
    "[Here](https://www.statisticssolutions.com/assumptions-of-linear-regression/) is a helpful resource on the assumptions of linear regression.\n",
    "\n",
    "Experiment: [Playing with regression line](https://www.desmos.com/calculator/jwquvmikhr) <br/>\n",
    "Limitations: [Anscombe's Quartet](https://www.desmos.com/calculator/paknt6oneh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using best_line\n",
    "\n",
    "best_line(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "The main idea here is pretty simple. Whereas, in simple linear regression we took our dependent variable to be a function only of a single independent variable, here we'll be taking the dependent variable to be a function of multiple independent variables.\n",
    "\n",
    "Our regression equation, then, instead of looking like $\\hat{y} = mx + b$, will now look like:\n",
    "\n",
    "$\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + ... + \\hat{\\beta}_nx_n$.\n",
    "\n",
    "Remember that the hats ( $\\hat{}$ ) indicate parameters that are estimated.\n",
    "\n",
    "Is this still a best-fit *line*? Well, no. What does the graph of, say, z = x + y look like? [Here's](https://academo.org/demos/3d-surface-plotter/) a 3d-plotter. (Of course, once we get x's with subscripts beyond 2 it's going to be very hard to visualize. But in practice linear regressions can make use of dozens or even of hundreds of independent variables!)\n",
    "\n",
    "I want to focus here more on what coding a multiple regression looks like in Python. But you might be wondering: Is it possible to calculate the betas by hand?\n",
    "\n",
    "Yes! See [here](https://stattrek.com/multiple-regression/regression-coefficients.aspx) for a nice explanation and example.\n",
    "\n",
    "We'll focus more directly on matrix mathematics later in the course.\n",
    "\n",
    "## Dealing with Categorical Variables\n",
    "\n",
    "One issue we'd like to resolve is what to do with categorical variables, i.e. variables that represent categories rather than continua. In a Pandas DataFrame, these columns may well have strings or objects for values, but they need not. A certain heart-disease dataset from Kaggle, for example, has a target variable that takes values 0-4, each representing a different stage of heart disease.\n",
    "\n",
    "### Dummying\n",
    "\n",
    "One very effective way of dealing with categorical variables is to dummy them out. What this involves is making a new column for _each categorical value in the column we're dummying out_.\n",
    "\n",
    "These new columns will be filled only with 0's and 1's, a 1 representing the presence of the relevant categorical value.\n",
    "\n",
    "Let's look at a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_use = pd.read_csv('data/comma-survey.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on this dataset see [here](https://fivethirtyeight.com/features/elitist-superfluous-or-popular-we-polled-americans-on-the-oxford-comma/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_use.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_use.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "comma_use.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_use.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try using sklearn's OneHotEncoder to create our dummy columns:\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(drop='first')\n",
    "comma_trans = ohe.fit_transform(comma_use.drop('RespondentID', axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could we have used ```pd.get_dummies()``` instead?\n",
    "\n",
    "Well, yes. And in fact ```get_dummies()``` is in some ways easier; for one thing, it's built right into Pandas. But there are drawbacks with it as well. See the *bottom* of [this link](https://stackoverflow.com/questions/36631163/pandas-get-dummies-vs-sklearns-onehotencoder-what-are-the-pros-and-cons) for a good explanation.\n",
    "\n",
    "So what did the encoder do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_trans.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(comma_trans.todense(), columns=ohe.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_csv('data/wine.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "Let's imagine that I'm going to try to predict wine quality based on the other features.\n",
    "\n",
    "Now: Which columns (predictors) should I choose? There are 12 predictors I could choose from For each of these predictors, I could either use it or not use it in my model, which means that there are 2^12 = 4096 different models I could construct! Well, okay, one of these is the \"empty model\" with no predictors in it. But there are still 4095 models from which I can choose.\n",
    "\n",
    "How can I decide which predictors to use in my model?\n",
    "\n",
    "We'll explore a few methods here. For more on feature selection, see [this post](https://towardsdatascience.com/the-5-feature-selection-algorithms-every-data-scientist-need-to-know-3a6b566efd2).\n",
    "\n",
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the .corr() DataFrame method to find out about the\n",
    "# correlation values between all pairs of variables!\n",
    "\n",
    "wine.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(8, 8)})\n",
    "\n",
    "# Use the .heatmap method to depict the relationships visually!\n",
    "sns.heatmap(wine.corr());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the correlations with 'quality'\n",
    "# (our dependent variable) in particular.\n",
    "\n",
    "wine.corr()['quality'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's choose 'alcohol' and 'density'.\n",
    "\n",
    "wine_preds = wine[['alcohol', 'density']]\n",
    "wine_target = wine['quality']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we construct a linear regression, let's *scale* our columns by z-scores. Why?\n",
    "\n",
    "In a word, it's useful to have all of our variables be on the same scale, so that the resulting coefficients are easier to interpret. If, moreover, the scales of the variables are very different one from another, then some of the coefficients may end up on very large or very tiny scales.\n",
    "\n",
    "For more on this, see [this post](https://stats.stackexchange.com/questions/32649/some-of-my-predictors-are-on-very-different-scales-do-i-need-to-transform-them).\n",
    "\n",
    "## Multiple Regression in StatsModels\n",
    "\n",
    "Statsmodels offers a highly descriptive report of the fit of a regression model. Let's generate a simple regression and then analyze the report!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's try data that fit a straight line perfectly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(20)\n",
    "y = 3*x + 1         # Note that we can do this only because x is a NumPy array!\n",
    "\n",
    "sm.OLS(y, sm.add_constant(x)).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rightarrow$Now let's add a little noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(20)\n",
    "y = [3*pt + 1 + gauss(mu=0, sigma=5) for pt in x]\n",
    "\n",
    "sm.OLS(y, sm.add_constant(x)).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we interpret this report?\n",
    "\n",
    "## Coefficient of Determination\n",
    "\n",
    "Very often a data scientist will calculate $R^2$, the *coefficient of determination*, as a measure of how well the model fits the data.\n",
    "\n",
    "$R^2$ for a model is ultimately a _relational_ notion. It's a measure of goodness of fit _relative_ to a (bad) baseline model. This bad baseline model is simply the horizontal line $y = \\mu_Y$, for dependent variable $Y$.\n",
    "\n",
    "The actual calculation of $R^2$ is: <br/> $\\Large R^2\\equiv 1-\\frac{\\Sigma_i(y_i - \\hat{y}_i)^2}{\\Sigma_i(y_i - \\bar{y})^2}$.\n",
    "\n",
    "$R^2$ is a measure of how much variation in the dependent variable your model explains.\n",
    "\n",
    "### Adjusted $R^2$\n",
    "\n",
    "There are some theoretical [objections](https://data.library.virginia.edu/is-r-squared-useless/) to using $R^2$ as an evaluator of a regression model.\n",
    "\n",
    "One objection is that, if we add another predictor to our model, $R^2$ can only *increase*! (It could hardly be that with more features I'd be able to account for *less* of the variation in the dependent variable than I could with the smaller set of features.)\n",
    "\n",
    "One improvement is **adjusted $R^2$**: <br/> $\\Large R^2_{adj.}\\equiv 1 - \\frac{(1 - R^2)(n - 1)}{n - m - 1}$, where:\n",
    "\n",
    "- n is the number of data points; and\n",
    "- m is the number of predictors.\n",
    "\n",
    "This can be a better indicator of the quality of a regression model. For more, see [here](https://www.statisticshowto.datasciencecentral.com/adjusted-r2/).\n",
    "\n",
    "## Other Regression Statistics\n",
    "\n",
    "What else do we have in this report?\n",
    "\n",
    "- **F-statistic**: The F-test measures the significance of your model relative to a model in which all coefficients are 0, i.e. relative to a model that says there is no correlation whatever between the predictors and the target. <br/><br/>\n",
    "- **Log-Likelihood**: The probability in question is the probability of seeing these data points, *given* the model parameter values. The higher this is, the more our data conform to our model and so the better our fit. AIC and BIC are related to the log-likelihood; we'll talk about those later. <br/><br/>\n",
    "- **coef**: These are the betas as calculated by the least-squares regression. We also have p-values and 95%-confidence intervals. <br/><br/>\n",
    "- **Omnibus**: This is a test for error normality. The probability is the chance that the errors are normally distributed. <br/><br/>\n",
    "- **Durbin-Watson**: This is a test for error homoskedasticity. We're looking for values between ~1.5 and ~2.5. <br/><br/>\n",
    "- **Jarque-Bera**: This is another test for error normality. <br/><br/>\n",
    "- **Cond. No.**: The condition number tests for independence of the predictors. Scores below 30 are ideal. When the predictors are *not* independent, we can run into problems of multicollinearity.\n",
    "\n",
    "For more on statsmodels regression statistics, see [here](https://www.accelebrate.com/blog/interpreting-results-from-linear-regression-is-the-data-appropriate).\n",
    "\n",
    "## Multicollinearity\n",
    "\n",
    "Multicollinearity describes the correlation between distinct predictors. Why might high multicollinearity be a problem for interpreting a linear regression model?\n",
    "\n",
    "Let's try a model with our wine dataset now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_preds_scaled = (wine_preds - np.mean(wine_preds)) / np.std(wine_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictors = sm.add_constant(wine_preds_scaled)\n",
    "model = sm.OLS(wine_target, predictors).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_preds2 = wine[['alcohol', 'density', 'volatile acidity']]\n",
    "\n",
    "wine_preds2_scaled = (wine_preds2 - np.mean(wine_preds2)) / np.std(wine_preds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = sm.add_constant(wine_preds2_scaled)\n",
    "model = sm.OLS(np.asarray(wine_target), predictors).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Regression in Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a StandardScaler object to scale our data for us.\n",
    "ss = StandardScaler()\n",
    "\n",
    "\n",
    "# Now we'll apply it to our data by using the .fit() and .transform() methods.\n",
    "\n",
    "ss.fit(wine_preds2)\n",
    "\n",
    "wine_preds2_scaled = ss.transform(wine_preds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can fit a LinearRegression object to our training data!\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(wine_preds2_scaled, wine_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the .coef_ attribute to recover the results\n",
    "# of the regression.\n",
    "\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination\n",
    "\n",
    "The idea behind recursive feature elimination is to start with all predictive features and then build down to a small set of features slowly, by eliminating the features with the lowest coefficients.\n",
    "\n",
    "That is:\n",
    "1. Start with a model with _all_ $n$ predictors;\n",
    "2. find the predictor with the smallest coefficient;\n",
    "3. throw that predictor out and build a model with the remining $n-1$ predictors;\n",
    "4. set $n = n-1$ and repeat until $n-1$ has the value you want!\n",
    "\n",
    "### Recursive Feature Elimination in Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "lr_rfe = LinearRegression()\n",
    "select = RFE(lr_rfe, n_features_to_select=1)\n",
    "select = select.fit(X = wine.drop('quality', axis=1), y = wine['quality'])\n",
    "\n",
    "select.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "select.ranking_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we can get better performance if we multiply features together. Consider the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv('data/Advertising.csv', index_col=0)\n",
    "\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to try to understand sales as a function of spending on various media (TV, radio, newspaper). We could check correlations of sales with the different features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.corr()['sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation with TV spending is pretty high. But look what happens when we add a new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['TV+Radio'] = sales['TV'] * sales['radio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.corr()['sales']['TV+Radio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation here is amazing! Let's make ourselves a plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sales['TV+Radio'], sales['sales']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, it's not easy to tell when such products of features will be so fruitful. Moreover, there is room for concern about violating regression's demand for feature independence. At the very least, we would probably not want to include a product *and the individual features themselves* in a final model, not if our goal is to understand what's really responsible for fluctuations in our target variable.\n",
    "\n",
    "## Sklearn Metrics\n",
    "\n",
    "The metrics module in sklearn has a number of metrics that we can use to meaure the accuracy of our model, including the $R^2$ score, the mean absolute error and the mean squared error. Note that the default 'score' on our model object is the $R^2$ score. Let's go back to our wine dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(wine_target, lr.predict(wine_preds2_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure this metric is properly calibrated. If we put simply $\\bar{y}$ as our prediction, then we should get an $R^2$ score of *0*. And if we predict, say, $\\bar{y} + 1$, then we should get a *negative* $R^2$ score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_quality = np.mean(wine_target)\n",
    "num = len(wine_target)\n",
    "\n",
    "metrics.r2_score(wine_target, avg_quality * np.ones(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(wine_target, (avg_quality + 1) * np.ones(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_absolute_error(Y_test, lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_squared_error(Y_test, lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "The power of a model comes from its **generality**: The linear (or hyper-linear) model implicitly gives us predictions for **any** value of the independent variable(s). And so we could imagine applying our model to **new** data that we haven't yet seen. Comparing the model's predictions with true values here would of course also make for a great way to test or to **validate** the model.\n",
    "\n",
    "After all, we can imagine that the data we have so far might have led to a model with certain drawbacks. Suppose our data has certain \"accidental\" correlations that are not likely to be repeated in new data. In that case the model would be highly sensitive to our data in ways that wouldn't generalize. In a word, the model would be picking up on **noise**, rather than genuine **signal**. We would expect the model to perform well for the data we used to construct it but not particularly well for other data. When a model has this sort of fluctuation we say that it has high **variance**, and that it is **overfit**.\n",
    "\n",
    "Note: Models can also be *underfit*. We'll have more to say about all this when we talk about **regularization** in a future lesson.\n",
    "\n",
    "Of course, new data can be hard to come by. But one thing we can do is to *shield* some of the data we already have from the construction of the model itself. If we isolate some of our data before building the model, we can then use that data later to validate it.\n",
    "\n",
    "This is what is meant by a **train-test split**. Divide the data in two parts: a training set and a test set, the first to construct (or *train*) the model, the second to validate (or *test*) the model.\n",
    "\n",
    "Of course, it makes most sense to split our data *randomly*. Scikit-Learn has a tool that will effect precisely this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(wine.drop('quality', axis=1), wine['quality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit our model to X_train and y_train, and then see how it performs on X_test and y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
